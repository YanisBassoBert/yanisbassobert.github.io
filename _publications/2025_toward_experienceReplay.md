---
title: "Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks"
collection: publications
category: manuscripts
permalink: /publication/2025_toward_experienceReplay
excerpt: "We present the first study enabling Class Incremental Learning (CIL) on Fully Binarized Neural Networks (FBNNs), pushing binary inference to its limits for ultra-low power edge devices. Our contributions include: (1) a CIL-compatible FBNN design and training pipeline, (2) loss balancing to manage forgetting, (3) semi-supervised pretraining for transferable features, and (4) a comparison of latent vs. native replay strategies. <br/>
  <img src='/images/schema-ter.svg' alt='Methodology schema' width='300'>"
date: 2024-10-01
venue: 'ArXiv'
paperurl: 'https://arxiv.org/pdf/2503.07107'
citation: 'Basso-Bert, Y., Molnos, A., Lemaire, R., Guicquero, W., & Dupret, A. (2025). Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks. arXiv preprint arXiv:2503.07107.'
---
Binary Neural Networks (BNNs) are a promising approach to enable Artificial Neural Network (ANN) implementation on ultra-low power edge devices. Such devices may compute data in highly dynamic environments, in which the classes targeted for inference can evolve or even novel classes may arise, requiring continual learning. Class Incremental Learning (CIL) is a common type of continual learning for classification problems, that has been scarcely addressed in the context of BNNs. Furthermore, most of existing BNNs models are not fully binary, as they require several real-valued network layers, at the input, the output, and for batch normalization. This paper goes a step further, enabling class incremental learning in Fully-Binarized NNs (FBNNs) through four main contributions. We firstly revisit the FBNN design and its training procedure that is suitable to CIL. Secondly, we explore loss balancing, a method to trade-off the performance of past and current classes. Thirdly, we propose a semi-supervised method to pre-train the feature extractor of the FBNN for transferable representations. Fourthly, two conventional CIL methods, \ie, Latent and Native replay, are thoroughly compared. These contributions are exemplified first on the CIFAR100 dataset, before being scaled up to address the CORE50 continual learning benchmark. The final results based on our 3Mb FBNN on CORE50 exhibit at par and better performance than conventional real-valued larger NN models.